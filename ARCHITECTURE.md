# GraNT Framework Architecture Diagram

```mermaid
graph TB
    subgraph "User Interface"
        A[Research Goal] --> B[AutoCognition Engine]
    end
    
    subgraph "Workflow Layer"
        B --> C[SEPA Engine]
        C --> D[Template Selection]
        D --> E[Solution Generation]
        E --> F[Outcome Tracking]
        F --> G[Learning Extraction]
        G --> H[Template Evolution]
        H --> C
    end
    
    subgraph "Core Mathematical Layer"
        E --> I[Granular Arithmetic]
        E --> J[Sheaf Attention]
        
        I --> K[Granule Operations]
        K --> K1["âŠ• Addition"]
        K --> K2["âŠ— Fusion"]
        K --> K3["â†“ Projection"]
        
        J --> L[Presheaf Construction]
        L --> M[Cocycle Optimization]
        M --> N[Global Sections]
    end
    
    subgraph "Neural Network Layer"
        N --> O[SheafTransformer]
        O --> P[Embedding Layer]
        P --> Q[Sheaf Attention Layers]
        Q --> R[Output Projection]
        
        Q --> Q1[Multi-Head Cocycle Attention]
        Q1 --> Q2[Residual Connections]
        Q2 --> Q3[Layer Normalization]
    end
    
    subgraph "Output & Deployment"
        R --> S[Solution Artifact]
        S --> T[Code Generation]
        S --> U[Documentation]
        S --> V[Proof Trace]
        S --> W[Performance Metrics]
        
        T --> X[Deploy to Production]
        X --> Y[Edge Devices]
        X --> Z[Cloud Services]
    end
    
    style A fill:#e1f5ff,stroke:#0288d1
    style B fill:#fff9c4,stroke:#f57c00
    style C fill:#f3e5f5,stroke:#7b1fa2
    style I fill:#e8f5e9,stroke:#388e3c
    style J fill:#e8f5e9,stroke:#388e3c
    style O fill:#fce4ec,stroke:#c2185b
    style S fill:#fff3e0,stroke:#e65100
```

## Component Descriptions

### User Interface Layer
- **Research Goal**: Natural language task specification with constraints and metrics

### Workflow Layer
- **AutoCognition Engine**: Main orchestrator
- **SEPA Engine**: Self-Evolving Prompt Architecture for adaptive templates
- **Template Selection**: Multi-armed bandit optimization
- **Solution Generation**: Autonomous architecture design
- **Outcome Tracking**: Persistent performance logging
- **Learning Extraction**: Pattern recognition from history
- **Template Evolution**: Continuous improvement loop

### Core Mathematical Layer
- **Granular Arithmetic**: 
  - âŠ• (Addition): Type-aware combination with confidence min
  - âŠ— (Fusion): Context-preserving aggregation with confidence product
  - â†“ (Projection): Lipschitz-bounded transformation with uncertainty propagation

- **Sheaf Attention**:
  - Presheaf Construction: Hierarchical feature organization
  - Cocycle Optimization: Minimize informational tension
  - Global Sections: Consistent cross-level aggregation

### Neural Network Layer
- **SheafTransformer**: Complete transformer architecture
- **Multi-Head Cocycle Attention**: Parallel attention heads with cohomological constraints
- **Residual Connections**: Skip connections for gradient flow
- **Layer Normalization**: Stable training dynamics

### Output & Deployment
- **Solution Artifact**: Complete package ready for deployment
  - Generated code (PyTorch modules)
  - Documentation (usage guides)
  - Proof traces (mathematical derivations)
  - Performance metrics (latency, memory, accuracy)

- **Deployment Targets**:
  - Edge devices (mobile, IoT)
  - Cloud services (AWS, GCP, Azure)

## Data Flow

```mermaid
sequenceDiagram
    participant User
    participant AutoCog as AutoCognition
    participant SEPA
    participant Granule as Granular Math
    participant Sheaf as Sheaf Attention
    participant Model as Neural Network
    
    User->>AutoCog: Submit Research Goal
    AutoCog->>SEPA: Select Template
    SEPA-->>AutoCog: Best Template (Îµ-greedy)
    
    AutoCog->>Granule: Create Data Granules
    Granule-->>AutoCog: Granule Space with Confidence
    
    AutoCog->>Sheaf: Build Architecture
    Sheaf->>Model: Instantiate SheafTransformer
    Model-->>Sheaf: Configured Model
    
    Sheaf-->>AutoCog: Complete Solution
    AutoCog->>SEPA: Record Outcome
    SEPA->>SEPA: Update Templates (Learn)
    
    AutoCog-->>User: Solution + Documentation
```

## Mathematical Framework

```mermaid
graph LR
    subgraph "Granule Space ğ’¢"
        G1["g = (x, Î¼, Ï„)"]
        G2["x âˆˆ X (value)"]
        G3["Î¼ âˆˆ [0,1] (confidence)"]
        G4["Ï„ âˆˆ T (type)"]
    end
    
    subgraph "Sheaf Theory"
        S1["F: P^op â†’ Vect"]
        S2["Ï_VU: F(U) â†’ F(V)"]
        S3["Î´: C^0 â†’ C^1 (coboundary)"]
        S4["Î± âˆˆ Z^1 (cocycle)"]
    end
    
    subgraph "Optimization"
        O1["E(Î±) = Î£ Î±_ij D_KL(f_j||f_i)"]
        O2["+ Î»H(Î±)"]
        O3["min E(Î±)"]
        O4["s.t. Î£_j Î±_ij = 1"]
    end
    
    G1 --> S1
    S1 --> S3
    S3 --> S4
    S4 --> O1
    O1 --> O3
    O2 --> O3
    
    style G1 fill:#e8f5e9
    style S1 fill:#e1f5ff
    style O3 fill:#fff9c4
```

## Deployment Pipeline

```mermaid
graph LR
    A[Source Code] --> B[Docker Build]
    B --> C{Target Platform}
    
    C -->|Edge| D[ONNX Export]
    D --> E[Quantization]
    E --> F[Mobile/IoT Deploy]
    
    C -->|Cloud| G[Container Registry]
    G --> H[Kubernetes Cluster]
    H --> I[Auto-scaling]
    
    C -->|Research| J[Jupyter Notebook]
    J --> K[Experiment Tracking]
    K --> L[Publication]
    
    style A fill:#e8f5e9
    style F fill:#fce4ec
    style I fill:#e1f5ff
    style L fill:#fff3e0
```

---

## Key Innovations Visualized

### 1. Uncertainty Propagation

```
Input Granule: gâ‚ = ([1,2,3], 0.9, VECTOR)
      â†“ (Lipschitz transformation L=1.5)
Project: normalize(Â·)
      â†“
Output: gâ‚‚ = ([0.27,0.53,0.80], 0.87, VECTOR)
                                  â†‘
                    Confidence decreased due to L
```

### 2. Cocycle Attention

```
Features: fâ‚, fâ‚‚, ..., fâ‚™
      â†“
Compute: D_KL(fâ±¼ || fáµ¢) for all pairs
      â†“
Optimize: Î±* = argmin Î£ Î±_ij D_KL + Î»H(Î±)
      â†“
Result: Î±_ij = softmax(-D_KL(fâ±¼||fáµ¢)/Î»)
```

### 3. Template Evolution

```
Iteration t: Template_v1 â†’ Execute â†’ Metrics â†’ Score
      â†“
Learning: Extract patterns from outcomes
      â†“
Evolution: Template_v2 = Template_v1 + Î”(patterns)
      â†“
Selection: Îµ-greedy choose between versions
      â†“
Iteration t+1: Best template â†’ Execute â†’ ...
```

---

This architecture enables:
âœ… End-to-end autonomous research
âœ… Mathematical rigor with practical efficiency
âœ… Continuous self-improvement
âœ… Production-ready deployment
